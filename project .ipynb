{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a0cb57",
   "metadata": {},
   "source": [
    "Text Analytics on online content.\n",
    "\n",
    "\n",
    "# Objective\n",
    "\n",
    "The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below. \n",
    "\n",
    "# Data Extraction\n",
    "\n",
    "Input.xlsx\n",
    "\n",
    "For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
    "\n",
    "While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2aed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212f5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e1074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://insights.blackcoffer.com/how-are-genet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-ai-use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://insights.blackcoffer.com/benefits-of-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://insights.blackcoffer.com/how-big-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-ai-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...\n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4       5  https://insights.blackcoffer.com/how-artificia...\n",
       "5       6  https://insights.blackcoffer.com/how-are-genet...\n",
       "6       7  https://insights.blackcoffer.com/how-is-ai-use...\n",
       "7       8  https://insights.blackcoffer.com/benefits-of-b...\n",
       "8       9  https://insights.blackcoffer.com/how-big-data-...\n",
       "9      10  https://insights.blackcoffer.com/how-will-ai-m..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cde809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfb4d774",
   "metadata": {},
   "source": [
    "We would derive the text content from each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7988651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c24462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://insights.blackcoffer.com/ai-and-its-impact-on-the-fashion-industry/'\n",
    "article = Article(url, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36aced82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sumant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd76874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download() \n",
    "article.parse() \n",
    "article.nlp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "216bf215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title:\n",
      "AI and its impact on the Fashion Industry\n",
      "\n",
      "\n",
      "Article Text:\n",
      "If you were a fan of the 90’s film Clueless back in the day, then you’ll remember the protagonist, Cher Horowitz’s amazing virtual wardrobe. She used it to browse her clothing and choose a perfectly coordinated ensemble. This virtual application, which was just the brainchild of a writer wanting to make the protagonist look rich, fashionable, and ahead of her time, ignited a buzz and prospected having an automated style device to make everyday dress-up fun and engagingly time-saving.\n",
      "\n",
      "Times have changed and technological advancement today is transforming everything from probabilities to possibilities. We are in the era where, machines not just facilitate our tasks and demands but rather suggest, forecasts, and analyze thus making lives simpler and smarter.\n",
      "\n",
      "With the advent of technology, style suggestions are just a fraction of the big picture that AI has painted in the fashion industry today.\n",
      "\n",
      "What is AI?\n",
      "\n",
      "Artificial intelligence is the creation of computer programs capable of performing activities and solving issues that would normally need human intelligence. AI has surged across a variety of industries, with the potential to transform businesses through creative technologies and more effective operational processes.\n",
      "\n",
      "At first, AI automation did not appear to be an appealing tool for fashion leaders to use in an industry focused on creative ability and expression. However, as we enter the hyper-digital era, these apps have the potential to revolutionize enterprises and generate considerable industry growth when compared to competitors that use traditional approaches.\n",
      "\n",
      "Some of the most well-known brands in the business are now investing in algorithms that assist buyers to choose designs. A slew of AI-based start-ups is also assisting everyone from retailers to customers in removing the guesswork from the equation.\n",
      "\n",
      "Impact on fashion\n",
      "\n",
      "This article examines how artificial intelligence has impacted key areas of the fashion industry, as well as explores brands that have benefited from its use.\n",
      "\n",
      "AI in Apparel designing\n",
      "\n",
      "Fashion firms are utilizing technology to better understand client wants and produce better garments thanks to more sophisticated data collection.\n",
      "\n",
      "Tommy Hilfiger pioneered the “Reimagine Retail” project, which trains fashion designers with AI design skills in collaboration with IBM. As a result, fashion students could master a variety of technical skills such as natural language processing (NLP) or computer vision. Fashion students could learn from thousands of fashion-related photos using AI, which increased their inventiveness and shortened lead times for the fashion firm.\n",
      "\n",
      "AI in the Manufacturing process\n",
      "\n",
      "Fashion brands are now able to identify fast-changing fashion trends and get the latest fashion accessories to store shelves faster than the “traditional” fashion shop, thanks to AI and machine learning capabilities. As a result, prominent fashion brands such as Zara, Top Shop, and H&M can provide immediate gratification to retail customers by identifying seasonal trends and producing the appropriate quantity of the current items.\n",
      "\n",
      "AI in Logistics\n",
      "\n",
      "AI in inventory and supply chain management helps to speed up processes by optimizing routes and lowering logistics and shipping costs. Companies use AI to automate logistics and supply chain procedures for speedier delivery or to locate alternate routes for vehicles that have been detoured due to unanticipated situations like bad weather or road construction.\n",
      "\n",
      "Owing to lockdown, major fashion firms had to rethink their go-to-market strategy overnight, with actual brick-and-mortar stores closed and people staying away from shopping malls. Myntra finds itself in a unique position to assist them.\n",
      "\n",
      "Myntra moved its whole data infrastructure, including supply chain management, inventory, and website capabilities, to Microsoft Azure just before the pandemic. Apart from giving Myntra the flexibility to respond to demand spikes, Azure’s built-in Machine Learning technologies sped up the development of advanced analytics capabilities, allowing them to better understand their customers.\n",
      "\n",
      "AI in Fashion Retail\n",
      "\n",
      "In retail, AI and machine learning are also providing an automated solution to monitor customer activities while shopping and visualize their sentiments to learn what products they prefer to buy and what products they ignore.\n",
      "\n",
      "AI can also track traffic in retail stores or record consumer shopping experiences, with the option of receiving feedback on how their experience was while shopping at the store, allowing them to enhance their services.\n",
      "\n",
      "Uniqlo has AI-powered UMood kiosks that offer clients a selection of products and use neurotransmitters to assess their reaction to color and style. The kiosk then makes product recommendations based on each person’s reactions. Customers don’t even need to press a button for the system to know how they feel about each item; their brain signals are plenty.\n",
      "\n",
      "AI Fashion Stylist — Styling the Fashion Accessories\n",
      "\n",
      "Furthermore, AI in fashion is enabling each of us to find those elusive perfect garments that suit our body types and fashion preferences.\n",
      "\n",
      "These AI-enabled garments and ensembles are personalized to the user’s style, body type, colors, and current fashion trends, as well as different situations and weather.\n",
      "\n",
      "iLUK is an AI-based personal stylist that uses Computer Vision and 3D Reconstruction technology at its core to provide technology-based personal styling. It’s shaped like a pod that will be placed in a store.\n",
      "\n",
      "AI in Fast Fashion with Smart Mirror\n",
      "\n",
      "Similarly, a smart mirror driven by AI is being utilized by a company to streamline consumers’ shopping experiences by allowing them to virtually visualize how items would appear on them without having to put them on their bodies.\n",
      "\n",
      "The AI smart mirror with touch screen glasses is mounted in the changing room of retail stores and relays information on whether or not a person is inside. Customers can also use this mirror to try on other sizes and colors, as well as receive tailored mix-and-match alternatives to complete the appearance.\n",
      "\n",
      "Van Heusen designed a storage space that includes a “Virtual Trial” mirror that allows customers to view how clothes might appear on them by scanning the item’s barcode and stepping in front of the mirror while virtual clothing is projected onto their reflection.\n",
      "\n",
      "AI in Ecommerce\n",
      "\n",
      "In the same way that AI is revolutionizing retail fashion stores, AI is revolutionizing online purchasing and E-commerce. While browsing or searching for fashion goods on e-commerce sites, AI suggests additional items that are similar to what you’re looking for based on your color preferences, budget, and other factors. It analyses your search history data and suggests more relevant stuff you should look at.\n",
      "\n",
      "Amazon has undoubtedly transformed the online shopping experience with its AI-powered product suggestion engine. Amazon is implementing an AI-enabled fashion designer algorithm that can create clothing by mimicking the design styles of several popular garments and applying them to a new garment. The Echo Look fashion assistant, which uses machine learning to deliver personalized recommendations, is Amazon’s other use case.\n",
      "\n",
      "AI in Visual Search — To Find the Products Using Camera\n",
      "\n",
      "AI-based visual search technology is now employed by E-commerce stores to comprehend the content and context of these photographs and produce a list of related results. You can capture an object using your camera and then search for it online. Retailers can use AI-enabled computer vision-based visual search technology to propose thematically or aesthetically relevant items to customers in a way that would be difficult to do with only a word query.\n",
      "\n",
      "Neiman Marcus, a high-end department store, employs artificial intelligence to make it easier for shoppers to locate things. The Snap. Find. Shop. the app allows users to photograph items they encounter while out and about, and then search Neiman Marcus’ inventory for the same or a comparable item.\n",
      "\n",
      "AI in Fashion and Sustainability\n",
      "\n",
      "One of the most damaging businesses on the earth is the fashion industry. Artificial Intelligence can be used in conjunction with Machine Learning, Deep Learning, Natural Language Processing, Visual Recognition, and Data Analytics to reduce trend prediction errors and more precisely forecast patterns, leading to fewer garments being manufactured and subsequently underutilized.\n",
      "\n",
      "The H&M Group is using “Amplified Intelligence,” which combines analytics and AI with human intelligence.\n",
      "\n",
      "Using artificially intelligent technologies, H&M is enhancing its ability to recognize trends and organize logistics, as well as minimizing the frequency of discounted deals and large amounts of unsold goods.\n",
      "\n",
      "Though intelligence per se is artificial, nevertheless is likely to have an earnest impact, both positive and negative:\n",
      "\n",
      "The fashion dilemma\n",
      "\n",
      "Apparel manufacture is a labor-intensive industry in the fashion industry. AI-enabled machines and robots can perfectly stitch fabrics while also detecting fabric flaws and providing quality assurance to ensure that the actual design hues will match the new colors.\n",
      "\n",
      "In nations like Bangladesh, where the garment industry accounts for 80% of the GDP, this will have a significant influence on the labor force in the long run. It is already feeling the heat from these specialized AI-based devices, thus jeopardizing the jobs of breadwinners.\n",
      "\n",
      "AI should not be viewed as a rival, but rather as a collaborator. The cost and ethics of AI are currently preventing us from progressing. We must be careful not to use technology to increase inequity or exacerbate social injustice. We also need to find a balance and integrate humanity into the machines we’re constructing in the future.\n",
      "\n",
      "Virtual wonderland\n",
      "\n",
      "“We are being monitored and classified by AI in portions of our lives that were not previously watched,” says Sophie Hackford, a futurist and keynote speaker. She wonders if we’ve built the “wrong” internet. We created it intending to monetize our viewers in mind, rather than the dynamic knowledge-sharing area that Tim Berners-Lee envisioned at the start.” She believes that in the future, we will be much more selective in how we obtain information, and that “bots” would be critical in meeting our requirements. As a result, the gains will come at the expense of our privacy.\n",
      "\n",
      "As numerous as AI’s advantages are, it is impossible to ignore the difficult issues it brings. With everything being data-driven, there is a pressing need to establish boundaries to build a shared ecosystem that benefits both businesses and the general public.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Finding a wise balance between physical stores and online stores will continue to be a crucial issue in an industry where brick-and-mortar retail locations are still an important element of the sector’s approach to business.\n",
      "\n",
      "The usefulness of chatbots aimed at improving online and in-store product navigation shows significant promise for preserving customer attention and use in the near term. We expect consumers to become accustomed to these capabilities over time, and the field to become more competitive as AI becomes more broadly applied, as prominent brands set the tone with their usage of AI.\n",
      "\n",
      "It’s too early to tell how these AI applications will affect earnings and cost savings because prominent fashion firms are still in the early stages of AI implementation. However, there will be a learning curve as corporations figure out how consumers react to these innovation efforts, which could have a direct influence on sales.\n",
      "\n",
      "Blackcoffer Insights 33: Lakshman Upadhyay and Prabhlin Kaur Matta, Welingkar Institute of M D R, Mumbai\n",
      "\n",
      "\n",
      "Article Summary:\n",
      "Impact on fashionThis article examines how artificial intelligence has impacted key areas of the fashion industry, as well as explores brands that have benefited from its use.\n",
      "AI Fashion Stylist — Styling the Fashion AccessoriesFurthermore, AI in fashion is enabling each of us to find those elusive perfect garments that suit our body types and fashion preferences.\n",
      "AI in EcommerceIn the same way that AI is revolutionizing retail fashion stores, AI is revolutionizing online purchasing and E-commerce.\n",
      "AI in Fashion and SustainabilityOne of the most damaging businesses on the earth is the fashion industry.\n",
      "Though intelligence per se is artificial, nevertheless is likely to have an earnest impact, both positive and negative:The fashion dilemmaApparel manufacture is a labor-intensive industry in the fashion industry.\n",
      "\n",
      "\n",
      "Article Keywords:\n",
      "['search', 'industry', 'retail', 'learning', 'intelligence', 'technology', 'shopping', 'fashion', 'impact', 'ai', 'stores']\n"
     ]
    }
   ],
   "source": [
    "print(\"Article Title:\") \n",
    "print(article.title) #prints the title of the article\n",
    "print(\"\\n\") \n",
    "print(\"Article Text:\") \n",
    "print(article.text) #prints the entire text of the article\n",
    "print(\"\\n\") \n",
    "print(\"Article Summary:\") \n",
    "print(article.summary) #prints the summary of the article\n",
    "print(\"\\n\") \n",
    "print(\"Article Keywords:\")\n",
    "print(article.keywords) #prints the keywords of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7441b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    \n",
    "    url1 = url\n",
    "    article = Article(url1, language=\"en\")\n",
    "    \n",
    "    article.download() \n",
    "    article.parse() \n",
    "    article.nlp()\n",
    "    \n",
    "    return article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa01eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumant\\AppData\\Local\\Temp\\ipykernel_24960\\869050490.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['URL_ID'][i] = get_text(df['URL'][i])\n",
      "C:\\Users\\Sumant\\AppData\\Local\\Temp\\ipykernel_24960\\869050490.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['URL_ID'][i] = get_text(df['URL'][i])\n",
      "C:\\Users\\Sumant\\AppData\\Local\\Temp\\ipykernel_24960\\869050490.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'When people hear AI they often think about sentient robots and magic boxes. AI today is much more mundane and simple—but that doesn’t mean it’s not powerful. Another misconception is that high-profile research projects can be applied directly to any business situation. AI done right can create an extreme return on investments (ROIs)—for instance through automation or precise prediction. But it does take thought, time, and proper implementation. We have seen that success and value generated by AI projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the C-suite down.\n",
      "\n",
      "“Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason and take action.”3 Lately there has been a big rise in the day-to-day use of machines powered by AI. These machines are wired using cross-disciplinary approaches based on mathematics, computer science, statistics, psychology, and more.4 Virtual assistants are becoming more common, most of the web shops predict your purchases, many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud.\n",
      "\n",
      "AI and Deep Learning technology employed in office entry systems will bring proper time tracking of each employee. As this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where Deep learning isn’t an algorithm per se, but rather a family of algorithms that implements deep networks (many layers). These networks are so deep that new methods of computation, such as graphics processing units (GPUs), are required to train them, in addition to clusters of compute nodes. So using deep learning we can take detect the employee using face and person recognition scan and through which login, logout timing is recorded. Using an AI system we can even identify each employee’s entry time, their working hours, non-working hours by tracking the movement of an employee in the office so that system can predict and report HR for the salary for each employee based on their working hours. Our system can take feed from CCTV to track movements of employees and this system is capable of recognizing a person even he/she is being masked as in this pandemic situation by taking their iris scan. With this system installed inside the office, the following are some of the benefits:\n",
      "\n",
      "1)Compliance/litigation needs\n",
      "\n",
      "For several countries, regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee. In the event of control from the labor inspectorate or a dispute with an employee, the employer must be able to explain and justify the working hours for the company. This can be made easy as our system is tracking employee movements\n",
      "\n",
      "2)Information security needs\n",
      "\n",
      "This is about monitoring user connection times to detect suspicious access times. In the event where compromised credentials are used to log on at 3 a.m. on a Saturday, a notification on this access could alert the IT team that an attack is possibly underway.\n",
      "\n",
      "3)Employee login logout software\n",
      "\n",
      "To manage and react to employees’ attendance, overtime thresholds, productivity, and suspicious access times, our system records and stores detailed and interactive reporting on users’ connection times. These records allow you to better manage users’ connection times and provide accurate, detailed data required by management.\n",
      "\n",
      "4)If you want to avoid paying overtime, make sure that your employees respect certain working time quotas or even avoid suspicious access. Our system will alert the HR officer about each employee’s office in and out time so that they can accordingly take action.\n",
      "\n",
      "5)Last but not least it reduces human resource needs to keep track of the records and sending the report to HR and HR officials has to check through the report so this system will reduce times and human resource needs\n",
      "\n",
      "With the use of AI and Deep Learning technologies, we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world.\n",
      "\n",
      "Blackcoffer Insights 33: Suriya E, Vellore Institute of Technology' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df['URL_ID'][i] = get_text(df['URL'][i])\n"
     ]
    },
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-do-deep-learning-models-predict-old-and-new-drugs-that-are-successfully-treated-in-healthcare/ on URL https://insights.blackcoffer.com/how-do-deep-learning-models-predict-old-and-new-drugs-that-are-successfully-treated-in-healthcare/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mURL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m, in \u001b[0;36mget_text\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      4\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(url1, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m article\u001b[38;5;241m.\u001b[39mdownload() \n\u001b[1;32m----> 7\u001b[0m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      8\u001b[0m article\u001b[38;5;241m.\u001b[39mnlp()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m article\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\Sumant\\Desktop\\mongodb\\Data-Extraction-and-Text-Analysis-main\\.venv\\Lib\\site-packages\\newspaper\\article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc)\n",
      "File \u001b[1;32mc:\\Users\\Sumant\\Desktop\\mongodb\\Data-Extraction-and-Text-Analysis-main\\.venv\\Lib\\site-packages\\newspaper\\article.py:531\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must `download()` an article first!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[1;31mArticleException\u001b[0m: Article `download()` failed with 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-do-deep-learning-models-predict-old-and-new-drugs-that-are-successfully-treated-in-healthcare/ on URL https://insights.blackcoffer.com/how-do-deep-learning-models-predict-old-and-new-drugs-that-are-successfully-treated-in-healthcare/"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(df)):\n",
    "    df['URL_ID'][i] = get_text(df['URL'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "314f7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'URL_ID':'Text'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d507ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://insights.blackcoffer.com/how-are-genet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-ai-use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://insights.blackcoffer.com/benefits-of-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://insights.blackcoffer.com/how-big-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-ai-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3                                                  4   \n",
       "4                                                  5   \n",
       "5                                                  6   \n",
       "6                                                  7   \n",
       "7                                                  8   \n",
       "8                                                  9   \n",
       "9                                                 10   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://insights.blackcoffer.com/how-is-login-...  \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...  \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...  \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...  \n",
       "4  https://insights.blackcoffer.com/how-artificia...  \n",
       "5  https://insights.blackcoffer.com/how-are-genet...  \n",
       "6  https://insights.blackcoffer.com/how-is-ai-use...  \n",
       "7  https://insights.blackcoffer.com/benefits-of-b...  \n",
       "8  https://insights.blackcoffer.com/how-big-data-...  \n",
       "9  https://insights.blackcoffer.com/how-will-ai-m...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf101e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce155586",
   "metadata": {},
   "source": [
    "Let's do some Text preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de24b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eea74cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sumant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e60847f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1222ad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Text Transform_Text\n",
      "0                    123            123\n",
      "1  This is a sample text    sample text\n",
      "2           Another text   another text\n",
      "3                    456            456\n"
     ]
    }
   ],
   "source": [
    "def transform(text):\n",
    "    if isinstance(text, str):  # Check if the input is already a string\n",
    "        review = text\n",
    "    else:\n",
    "        review = str(text)  # Convert non-string values to string\n",
    "    \n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review)  # Keep only alphanumeric characters\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "    review = review.split()  # Split text into words\n",
    "\n",
    "    # Remove stopwords\n",
    "    review = [word for word in review if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "# Create a sample DataFrame (replace this with your actual DataFrame)\n",
    "data = {'Text': [123, 'This is a sample text', 'Another text', 456]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply the transformation function to the 'Text' column\n",
    "df['Transform_Text'] = df['Text'].apply(transform)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4b78ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def transform(text):\\n\\n    review = re.sub('[^a-zA-Z0-9]', ' ',text)  # except small and capital letters and numeric remove everythong.\\n    review = review.lower()                    # lower it.\\n    review = review.split()\\n    \\n    review = [word for word in review if not word in stopwords.words('english')]   # remove stopwords.\\n    review = ' '.join(review)\\n    return review\\n\\n\\ndf['Transform_Text'] = df['Text'].apply(transform)\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def transform(text):\n",
    "\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ',text)  # except small and capital letters and numeric remove everythong.\n",
    "    review = review.lower()                    # lower it.\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [word for word in review if not word in stopwords.words('english')]   # remove stopwords.\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "\n",
    "df['Transform_Text'] = df['Text'].apply(transform)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73b213",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "For each of the extracted texts from the article, perform textual analysis and compute variables.\n",
    "\n",
    "I am looking for these variables in the analysis document:\n",
    "\n",
    "POSITIVE SCORE\n",
    "\n",
    "NEGATIVE SCORE\n",
    "\n",
    "POLARITY SCORE\n",
    "\n",
    "SUBJECTIVITY SCORE\n",
    "\n",
    "AVG SENTENCE LENGTH\n",
    "\n",
    "PERCENTAGE OF COMPLEX WORDS\n",
    "\n",
    "FOG INDEX\n",
    "\n",
    "AVG NUMBER OF WORDS PER SENTENCE\n",
    "\n",
    "COMPLEX WORD COUNT\n",
    "\n",
    "WORD COUNT\n",
    "\n",
    "SYLLABLE PER WORD\n",
    "\n",
    "PERSONAL PRONOUNS\n",
    "\n",
    "AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d44ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count in each text row.\n",
    "\n",
    "df['word_counts'] = df['Transform_Text'].apply(lambda x: len(str(x).split()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a43046bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "073bd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sumant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e869db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame (replace this with your actual DataFrame)\n",
    "data = {'Text': [123, 'This is a sample text', 'Another text', 456]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert non-string values in the 'Text' column to strings\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "# Check the length of the tokenized sentences\n",
    "for text in df['Text']:\n",
    "    print(len(nltk.sent_tokenize(text)))  # Tokenize each text and get the length of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3d60324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.sent_tokenize(df['Text'][0]))  # checking length function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2059a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4451e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "679b250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sumant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd065899",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'word_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Sumant\\Desktop\\mongodb\\Data-Extraction-and-Text-Analysis-main\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word_counts'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage number of words per sentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m----> 5\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage number of words per sentence\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword_counts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(nltk\u001b[38;5;241m.\u001b[39msent_tokenize(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m][i]))\n",
      "File \u001b[1;32mc:\\Users\\Sumant\\Desktop\\mongodb\\Data-Extraction-and-Text-Analysis-main\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Sumant\\Desktop\\mongodb\\Data-Extraction-and-Text-Analysis-main\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word_counts'"
     ]
    }
   ],
   "source": [
    "df['average number of words per sentence'] = np.nan\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    \n",
    "    df['average number of words per sentence'][i] = df['word_counts'][i]/len(nltk.sent_tokenize(df['Text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4d155f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a sample sentence.</td>\n",
       "      <td>[This is a sample sentence.]</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another sentence.</td>\n",
       "      <td>[Another sentence.]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yet another sentence.</td>\n",
       "      <td>[Yet another sentence.]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Text                     sentences  sentence_count  \\\n",
       "0  This is a sample sentence.  [This is a sample sentence.]               1   \n",
       "1           Another sentence.           [Another sentence.]               1   \n",
       "2       Yet another sentence.       [Yet another sentence.]               1   \n",
       "\n",
       "   word_count  avg_words_per_sentence  average number of words per sentence  \n",
       "0           6                     6.0                                   NaN  \n",
       "1           3                     3.0                                   NaN  \n",
       "2           4                     4.0                                   NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1d32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ed03315",
   "metadata": {},
   "source": [
    "# Average Word Length\n",
    "\n",
    "\n",
    "Average Word Length is calculated by the formula:\n",
    "    \n",
    "( Sum of the total number of characters in each word ) / ( Total number of words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34028754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(x):\n",
    "    s = x.split()\n",
    "    x = ''.join(s)\n",
    "    return len(x)      # counting the total number of characters in each text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e20965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chara_count'] = df['Transform_Text'].apply(lambda x: char_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1417dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06352e73",
   "metadata": {},
   "source": [
    "to check for stopwords in each text.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df['stopwords'] = df['Text'].apply(lambda x: [t for t in x.split() if t  in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2823a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-2b95d69bd553>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['average word length'][i] = df['chara_count'][i]/df['word_counts'][i]\n"
     ]
    }
   ],
   "source": [
    "df['average word length'] = np.nan\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    \n",
    "    df['average word length'][i] = df['chara_count'][i]/df['word_counts'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb832e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \n",
       "0                             18.083333         2780             6.405530  \n",
       "1                             13.678571         2618             6.835509  \n",
       "2                             14.324675         7515             6.813237  \n",
       "3                             16.933333         1778             7.000000  \n",
       "4                             10.611940         4640             6.526020  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24881903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d00ed3c",
   "metadata": {},
   "source": [
    "# SYLLABLE COUNT\n",
    "\n",
    "We count the number of Syllables in each word of the text by counting the vowels present in each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846352ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428df636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc932be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 5, 'e': 3, 'i': 4, 'a': 3, 'u': 1}\n",
      "['o', 'e', 'i', 'a', 'o', 'e', 'o', 'o', 'o', 'i', 'e', 'a', 'i', 'i', 'a', 'u']\n",
      "[5, 3, 4, 3, 1]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "def syllable_count(x):\n",
    "    v = []\n",
    "    d = {}\n",
    "    for i in x:\n",
    "        if i in \"aeiou\":\n",
    "            v.append(i)\n",
    "            d[i] = d.get(i,0)+1     # checking purpose\n",
    "            \n",
    "    k = []\n",
    "    for i in d:\n",
    "        k.append(d[i])\n",
    "    print(d)\n",
    "    print(v)  \n",
    "    print(k)\n",
    "    print(np.sum(k))\n",
    "        \n",
    "    \n",
    "g = 'bore i am gone to london in england britian uk'\n",
    "\n",
    "syllable_count(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8541dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8975f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def syllable_count(x):\n",
    "    v = []\n",
    "    d = {}\n",
    "    for i in x:\n",
    "        if i in \"aeiou\":\n",
    "            v.append(i)\n",
    "            d[i] = d.get(i,0)+1\n",
    "            \n",
    "    k = []\n",
    "    for i in d:\n",
    "        k.append(d[i])\n",
    "\n",
    "    return np.sum(k)\n",
    "\n",
    "g = h['Transform_Text'][1]\n",
    "\n",
    "syllable_count(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff21ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d33be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['syllable count'] = df['Transform_Text'].apply(lambda x: syllable_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  \n",
       "0            1074  \n",
       "1             996  \n",
       "2            2904  \n",
       "3             694  \n",
       "4            1840  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde7e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ffdda03",
   "metadata": {},
   "source": [
    "# COMPLEX Word Count\n",
    "\n",
    "Complex words are words in the text that contain more than two Syllables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cab496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import  Counter\n",
    "\n",
    "def complex_word_count(x):\n",
    "    \n",
    "    syllable = 'aeiou'\n",
    "    \n",
    "    t = x.split()\n",
    "    \n",
    "    v = []\n",
    "    \n",
    "    for i in t:\n",
    "        words = i.split()\n",
    "        c=Counter()\n",
    "        \n",
    "        for word in words:\n",
    "            c.update(set(word))\n",
    "\n",
    "        n = 0\n",
    "        for a in c.most_common():\n",
    "            if a[0] in syllable:\n",
    "                if a[1] >= 2:\n",
    "                    n += 1\n",
    "                \n",
    "        m = 0\n",
    "        p = []\n",
    "        for a in c.most_common():\n",
    "            if a[0] in syllable:\n",
    "                p.append(a[0])\n",
    "        if len(p) >= 2:\n",
    "            m += 1\n",
    "        \n",
    "        if n >= 1 or m >= 1:\n",
    "            v.append(i)\n",
    "            \n",
    "    return len(v) \n",
    "\n",
    "g = h['Transform_Text'][1]\n",
    "\n",
    "complex_word_count(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8097f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d682973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  \n",
       "0            1074            321  \n",
       "1             996            293  \n",
       "2            2904            884  \n",
       "3             694            195  \n",
       "4            1840            553  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['complex_count'] = np.nan\n",
    "\n",
    "df['complex_count'] = df['Transform_Text'].apply(lambda x: complex_word_count(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad55ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "477aa6a8",
   "metadata": {},
   "source": [
    "# Analysis of Readability\n",
    "\n",
    "Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
    "\n",
    "Average Sentence Length      =  the number of words / the number of sentences\n",
    "\n",
    "Percentage of Complex words  =  the number of complex words / the number of words \n",
    "\n",
    "Fog Index                    =  0.4 * (Average Sentence Length + Percentage of Complex words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75245957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-b2c86d949a96>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentence length'][i]  =   len(nltk.sent_tokenize(df['Text'][i]))\n",
      "<ipython-input-32-b2c86d949a96>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Average Sentence Length'][i] = df['word_counts'][i]/df['sentence length'][i]\n",
      "<ipython-input-32-b2c86d949a96>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Percentage of Complex words'][i] = df['complex_count'][i]/df['word_counts'][i]\n",
      "<ipython-input-32-b2c86d949a96>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fog Index'][i] = 0.4 * (df['Average Sentence Length'][i] + df['Percentage of Complex words'][i])\n"
     ]
    }
   ],
   "source": [
    "df['sentence length'] = np.nan\n",
    "df['Average Sentence Length'] = np.nan\n",
    "df['Percentage of Complex words'] = np.nan\n",
    "df['Fog Index'] = np.nan\n",
    "\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    \n",
    "    df['sentence length'][i]  =   len(nltk.sent_tokenize(df['Text'][i]))\n",
    "    df['Average Sentence Length'][i] = df['word_counts'][i]/df['sentence length'][i]\n",
    "    df['Percentage of Complex words'][i] = df['complex_count'][i]/df['word_counts'][i] \n",
    "    df['Fog Index'][i] = 0.4 * (df['Average Sentence Length'][i] + df['Percentage of Complex words'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6244cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  \n",
       "0                     0.739631   7.529186  \n",
       "1                     0.765013   5.777434  \n",
       "2                     0.801451   6.050450  \n",
       "3                     0.767717   7.080420  \n",
       "4                     0.777778   4.555887  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1bbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491f016b",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "Sentimental analysis is the process of determining whether a piece of writing is positive, negative or neutral.\n",
    "\n",
    "The Master Dictionary (found here) is used for creating a dictionary of Positive and Negative words. We add only those words in the dictionary if they are not found in the Stop Words Lists. Use this url if above does not work https://sraf.nd.edu/textual-analysis/resources/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb321d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('sentiment dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdb42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86526</th>\n",
       "      <td>ZYGOTE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86527</th>\n",
       "      <td>ZYGOTES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86528</th>\n",
       "      <td>ZYGOTIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86529</th>\n",
       "      <td>ZYMURGIES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86530</th>\n",
       "      <td>ZYMURGY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86531 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Negative  Positive\n",
       "0       AARDVARK         0         0\n",
       "1      AARDVARKS         0         0\n",
       "2          ABACI         0         0\n",
       "3          ABACK         0         0\n",
       "4         ABACUS         0         0\n",
       "...          ...       ...       ...\n",
       "86526     ZYGOTE         0         0\n",
       "86527    ZYGOTES         0         0\n",
       "86528    ZYGOTIC         0         0\n",
       "86529  ZYMURGIES         0         0\n",
       "86530    ZYMURGY         0         0\n",
       "\n",
       "[86531 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = sentiment[['Word','Negative','Positive']]\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb10bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "f = ['ZYGOTIC','BAD','DONE','EXCELLENT','WORSE']\n",
    "\n",
    "negative = 0\n",
    "positive = 0\n",
    "\n",
    "for i in dfs['Word']:\n",
    "    if i in f:\n",
    "        if dfs[dfs['Word']==i].Negative.any() == True:\n",
    "            negative += 1\n",
    "        if dfs[dfs['Word']==i].Positive.any() == True:                # CHECKING\n",
    "            positive += 1\n",
    "            \n",
    "print(negative),\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f375db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd5b7bd",
   "metadata": {},
   "source": [
    "We need to lower the word column in dfs to be used for sentiment score for the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a200ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a667221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word        0\n",
       "Negative    0\n",
       "Positive    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = dfs.dropna()\n",
    "dfs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4272e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'good', 'man']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'the good man'\n",
    "w.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['word_lower'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f81d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "50741",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 50741",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5b2bd7f0983d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_lower'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 50741"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "        dfs['word_lower'][i] = dfs['Word'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50742,len(dfs)):\n",
    "        dfs['word_lower'][i] = dfs['Word'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e0cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['word_lower'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69831ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>word_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aardvark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aardvarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abaci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abacus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Negative  Positive word_lower\n",
       "0   AARDVARK         0         0   aardvark\n",
       "1  AARDVARKS         0         0  aardvarks\n",
       "2      ABACI         0         0      abaci\n",
       "3      ABACK         0         0      aback\n",
       "4     ABACUS         0         0     abacus"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fefbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bbfc79",
   "metadata": {},
   "source": [
    "# Positive Score\n",
    "\n",
    "Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f8dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the positive score for text.\n",
    "\n",
    "def positive(x):\n",
    "    \n",
    "    s = x.split()\n",
    "    \n",
    "    positive = 0\n",
    "    \n",
    "    for i in dfs['word_lower']:\n",
    "        if i in s:\n",
    "            if dfs[dfs['word_lower']==i].Positive.any() == True:\n",
    "                positive += 1\n",
    "            \n",
    "    return positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce285e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive_score'] = np.nan\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['positive_score'][i] = positive(df['Transform_Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  \n",
       "0                     0.739631   7.529186             4.0  \n",
       "1                     0.765013   5.777434             9.0  \n",
       "2                     0.801451   6.050450            27.0  \n",
       "3                     0.767717   7.080420             5.0  \n",
       "4                     0.777778   4.555887            19.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8296c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_word(x):\n",
    "    \n",
    "    s = x.split()\n",
    "    \n",
    "    positive_word = []\n",
    "    \n",
    "    for i in dfs['word_lower']:\n",
    "        if i in s:\n",
    "            if dfs[dfs['word_lower']==i].Positive.any() == True:   # checking which words are positive\n",
    "                positive_word.append(i)\n",
    "            \n",
    "    print(positive_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e1c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'better', 'easy', 'success']\n"
     ]
    }
   ],
   "source": [
    "df['positive_word'] = np.nan\n",
    "\n",
    "for i in range(1):\n",
    "    df['positive_word'][i] = positive_word(df['Transform_Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('positive_word',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf5f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d3d0041",
   "metadata": {},
   "source": [
    "# NEGATIVE Score\n",
    "\n",
    "Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_score(x):\n",
    "    \n",
    "    s = x.split()\n",
    "    \n",
    "    negative = 0\n",
    "    \n",
    "    for i in dfs['word_lower']:\n",
    "        if i in s:\n",
    "            if dfs[dfs['word_lower']==i].Negative.any() == True:\n",
    "                negative += 1\n",
    "            \n",
    "    return negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negative_score'] = np.nan\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['negative_score'][i] = negative_score(df['Transform_Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d1b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3159d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  negative_score  \n",
       "0                     0.739631   7.529186             4.0             5.0  \n",
       "1                     0.765013   5.777434             9.0             6.0  \n",
       "2                     0.801451   6.050450            27.0            21.0  \n",
       "3                     0.767717   7.080420             5.0             1.0  \n",
       "4                     0.777778   4.555887            19.0            17.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa4770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf5f9a04",
   "metadata": {},
   "source": [
    "# Polarity Score\n",
    "\n",
    "\n",
    "Polarity Score: This is the score that determines if a given text is positive or negative in nature.\n",
    "\n",
    "It is calculated by using the formula: \n",
    "\n",
    "Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "\n",
    "Range is from -1 to +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Polarity Score'] = np.nan\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['Polarity Score'][i] = (df['positive_score'][i]-df['negative_score'][i])/ ((df['positive_score'][i] + df['negative_score'][i]) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598f078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  negative_score  \\\n",
       "0                     0.739631   7.529186             4.0             5.0   \n",
       "1                     0.765013   5.777434             9.0             6.0   \n",
       "2                     0.801451   6.050450            27.0            21.0   \n",
       "3                     0.767717   7.080420             5.0             1.0   \n",
       "4                     0.777778   4.555887            19.0            17.0   \n",
       "\n",
       "   Polarity Score  \n",
       "0       -0.111111  \n",
       "1        0.200000  \n",
       "2        0.125000  \n",
       "3        0.666667  \n",
       "4        0.055556  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ceb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f3dfa05",
   "metadata": {},
   "source": [
    "# SUBJECTIVITY SCORE\n",
    "\n",
    "Subjectivity Score: This is the score that determines if a given text is objective or subjective. \n",
    "\n",
    "It is calculated by using the formula: \n",
    "\n",
    "Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "\n",
    "Range is from 0 to +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6864d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.13926077545642765, subjectivity=0.48480990024468285)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(df['Transform_Text'][1])\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfa2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48480990024468285"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(df['Transform_Text'][1]).sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac78c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity'] = np.nan\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['subjectivity'][i] = TextBlob(df['Transform_Text'][i]).sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e2dab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.459438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.484810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.542281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.451277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.483817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  negative_score  \\\n",
       "0                     0.739631   7.529186             4.0             5.0   \n",
       "1                     0.765013   5.777434             9.0             6.0   \n",
       "2                     0.801451   6.050450            27.0            21.0   \n",
       "3                     0.767717   7.080420             5.0             1.0   \n",
       "4                     0.777778   4.555887            19.0            17.0   \n",
       "\n",
       "   Polarity Score  subjectivity  \n",
       "0       -0.111111      0.459438  \n",
       "1        0.200000      0.484810  \n",
       "2        0.125000      0.542281  \n",
       "3        0.666667      0.451277  \n",
       "4        0.055556      0.483817  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13acb603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "940074f8",
   "metadata": {},
   "source": [
    "# PERSONAL PRONOUNS \n",
    "\n",
    "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb725d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e750ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "the my father\n"
     ]
    }
   ],
   "source": [
    "x = 'he is the my father'\n",
    "y = nlp(x)\n",
    "\n",
    "for noun in y.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('he is the my father')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24963b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'PRON':\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSONAL PRONOUNS'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6caf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[their,\n",
       " them,\n",
       " their,\n",
       " it,\n",
       " them,\n",
       " its,\n",
       " it,\n",
       " Our,\n",
       " their,\n",
       " What,\n",
       " them,\n",
       " they,\n",
       " it,\n",
       " Our,\n",
       " your,\n",
       " It,\n",
       " Our,\n",
       " your,\n",
       " they,\n",
       " their,\n",
       " it,\n",
       " we,\n",
       " we,\n",
       " them,\n",
       " it,\n",
       " our,\n",
       " it,\n",
       " it,\n",
       " our]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(df['Text'][1])\n",
    "tok = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'PRON':\n",
    "        tok.append(token)\n",
    "        \n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da744492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSONAL PRONOUNS'][1] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c48006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.459438</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.484810</td>\n",
       "      <td>[their, them, their, it, them, its, it, Our, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.542281</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.451277</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.483817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  negative_score  \\\n",
       "0                     0.739631   7.529186             4.0             5.0   \n",
       "1                     0.765013   5.777434             9.0             6.0   \n",
       "2                     0.801451   6.050450            27.0            21.0   \n",
       "3                     0.767717   7.080420             5.0             1.0   \n",
       "4                     0.777778   4.555887            19.0            17.0   \n",
       "\n",
       "   Polarity Score  subjectivity  \\\n",
       "0       -0.111111      0.459438   \n",
       "1        0.200000      0.484810   \n",
       "2        0.125000      0.542281   \n",
       "3        0.666667      0.451277   \n",
       "4        0.055556      0.483817   \n",
       "\n",
       "                                   PERSONAL PRONOUNS  \n",
       "0                                                NaN  \n",
       "1  [their, them, their, it, them, its, it, Our, t...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4067d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSONAL PRONOUNS'] = np.nan\n",
    "\n",
    "for i in range(len(df)):\n",
    "    doc = nlp(df['Text'][i])\n",
    "    tok = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            tok.append(token)\n",
    "        \n",
    "    df['PERSONAL PRONOUNS'][i] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c040b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "      <th>Transform_Text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>chara_count</th>\n",
       "      <th>average word length</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>sentence length</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>people hear ai often think sentient robots mag...</td>\n",
       "      <td>434</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>2780</td>\n",
       "      <td>6.405530</td>\n",
       "      <td>1074</td>\n",
       "      <td>321</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.459438</td>\n",
       "      <td>[they, it, it, We, there, what, their, there, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>increasing computing power data potential valu...</td>\n",
       "      <td>383</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>2618</td>\n",
       "      <td>6.835509</td>\n",
       "      <td>996</td>\n",
       "      <td>293</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.484810</td>\n",
       "      <td>[their, them, their, it, them, its, it, Our, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>fan 90 film clueless back day remember protago...</td>\n",
       "      <td>1103</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>7515</td>\n",
       "      <td>6.813237</td>\n",
       "      <td>2904</td>\n",
       "      <td>884</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.542281</td>\n",
       "      <td>[you, you, She, it, her, her, everything, We, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>understanding exactly data ingested analyzed r...</td>\n",
       "      <td>254</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>1778</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>694</td>\n",
       "      <td>195</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.451277</td>\n",
       "      <td>[they, they, its, they, their, it, its, we, them]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>711</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>4640</td>\n",
       "      <td>6.526020</td>\n",
       "      <td>1840</td>\n",
       "      <td>553</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.483817</td>\n",
       "      <td>[we, what, I, we, our, we, us, our, what, We, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  When people hear AI they often think about sen...   \n",
       "1  With increasing computing power and more data,...   \n",
       "2  If you were a fan of the 90’s film Clueless ba...   \n",
       "3  Understanding exactly how data is ingested, an...   \n",
       "4  From the stone age to the modern world, from h...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4  https://insights.blackcoffer.com/how-artificia...   \n",
       "\n",
       "                                      Transform_Text  word_counts  \\\n",
       "0  people hear ai often think sentient robots mag...          434   \n",
       "1  increasing computing power data potential valu...          383   \n",
       "2  fan 90 film clueless back day remember protago...         1103   \n",
       "3  understanding exactly data ingested analyzed r...          254   \n",
       "4  stone age modern world hunting gathering culti...          711   \n",
       "\n",
       "   average number of words per sentence  chara_count  average word length  \\\n",
       "0                             18.083333         2780             6.405530   \n",
       "1                             13.678571         2618             6.835509   \n",
       "2                             14.324675         7515             6.813237   \n",
       "3                             16.933333         1778             7.000000   \n",
       "4                             10.611940         4640             6.526020   \n",
       "\n",
       "   syllable count  complex_count  sentence length  Average Sentence Length  \\\n",
       "0            1074            321             24.0                18.083333   \n",
       "1             996            293             28.0                13.678571   \n",
       "2            2904            884             77.0                14.324675   \n",
       "3             694            195             15.0                16.933333   \n",
       "4            1840            553             67.0                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  positive_score  negative_score  \\\n",
       "0                     0.739631   7.529186             4.0             5.0   \n",
       "1                     0.765013   5.777434             9.0             6.0   \n",
       "2                     0.801451   6.050450            27.0            21.0   \n",
       "3                     0.767717   7.080420             5.0             1.0   \n",
       "4                     0.777778   4.555887            19.0            17.0   \n",
       "\n",
       "   Polarity Score  subjectivity  \\\n",
       "0       -0.111111      0.459438   \n",
       "1        0.200000      0.484810   \n",
       "2        0.125000      0.542281   \n",
       "3        0.666667      0.451277   \n",
       "4        0.055556      0.483817   \n",
       "\n",
       "                                   PERSONAL PRONOUNS  \n",
       "0  [they, it, it, We, there, what, their, there, ...  \n",
       "1  [their, them, their, it, them, its, it, Our, t...  \n",
       "2  [you, you, She, it, her, her, everything, We, ...  \n",
       "3  [they, they, its, they, their, it, its, we, them]  \n",
       "4  [we, what, I, we, our, we, us, our, what, We, ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be6ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[you,\n",
       " you,\n",
       " She,\n",
       " it,\n",
       " her,\n",
       " her,\n",
       " everything,\n",
       " We,\n",
       " our,\n",
       " What,\n",
       " we,\n",
       " everyone,\n",
       " its,\n",
       " their,\n",
       " their,\n",
       " itself,\n",
       " them,\n",
       " its,\n",
       " them,\n",
       " their,\n",
       " their,\n",
       " they,\n",
       " they,\n",
       " their,\n",
       " them,\n",
       " their,\n",
       " their,\n",
       " they,\n",
       " their,\n",
       " us,\n",
       " our,\n",
       " its,\n",
       " It,\n",
       " them,\n",
       " them,\n",
       " them,\n",
       " their,\n",
       " them,\n",
       " their,\n",
       " what,\n",
       " you,\n",
       " your,\n",
       " It,\n",
       " your,\n",
       " you,\n",
       " its,\n",
       " them,\n",
       " You,\n",
       " your,\n",
       " it,\n",
       " it,\n",
       " they,\n",
       " its,\n",
       " It,\n",
       " us,\n",
       " We,\n",
       " We,\n",
       " we,\n",
       " We,\n",
       " our,\n",
       " She,\n",
       " we,\n",
       " We,\n",
       " it,\n",
       " our,\n",
       " She,\n",
       " we,\n",
       " we,\n",
       " our,\n",
       " our,\n",
       " it,\n",
       " it,\n",
       " everything,\n",
       " there,\n",
       " We,\n",
       " their,\n",
       " It,\n",
       " there]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PERSONAL PRONOUNS'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c6ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df[['URL','positive_score','negative_score','Polarity Score','subjectivity','Average Sentence Length','Percentage of Complex words',\n",
    "            'Fog Index','average number of words per sentence','complex_count','word_counts','syllable count','PERSONAL PRONOUNS','average word length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee78f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>average number of words per sentence</th>\n",
       "      <th>complex_count</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>syllable count</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>average word length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.459438</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>0.739631</td>\n",
       "      <td>7.529186</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>321</td>\n",
       "      <td>434</td>\n",
       "      <td>1074</td>\n",
       "      <td>[they, it, it, We, there, what, their, there, ...</td>\n",
       "      <td>6.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.484810</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>0.765013</td>\n",
       "      <td>5.777434</td>\n",
       "      <td>13.678571</td>\n",
       "      <td>293</td>\n",
       "      <td>383</td>\n",
       "      <td>996</td>\n",
       "      <td>[their, them, their, it, them, its, it, Our, t...</td>\n",
       "      <td>6.835509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.542281</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>6.050450</td>\n",
       "      <td>14.324675</td>\n",
       "      <td>884</td>\n",
       "      <td>1103</td>\n",
       "      <td>2904</td>\n",
       "      <td>[you, you, She, it, her, her, everything, We, ...</td>\n",
       "      <td>6.813237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.451277</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>7.080420</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>195</td>\n",
       "      <td>254</td>\n",
       "      <td>694</td>\n",
       "      <td>[they, they, its, they, their, it, its, we, them]</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.483817</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>4.555887</td>\n",
       "      <td>10.611940</td>\n",
       "      <td>553</td>\n",
       "      <td>711</td>\n",
       "      <td>1840</td>\n",
       "      <td>[we, what, I, we, our, we, us, our, what, We, ...</td>\n",
       "      <td>6.526020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  positive_score  \\\n",
       "0  https://insights.blackcoffer.com/how-is-login-...             4.0   \n",
       "1  https://insights.blackcoffer.com/how-does-ai-h...             9.0   \n",
       "2  https://insights.blackcoffer.com/ai-and-its-im...            27.0   \n",
       "3  https://insights.blackcoffer.com/how-do-deep-l...             5.0   \n",
       "4  https://insights.blackcoffer.com/how-artificia...            19.0   \n",
       "\n",
       "   negative_score  Polarity Score  subjectivity  Average Sentence Length  \\\n",
       "0             5.0       -0.111111      0.459438                18.083333   \n",
       "1             6.0        0.200000      0.484810                13.678571   \n",
       "2            21.0        0.125000      0.542281                14.324675   \n",
       "3             1.0        0.666667      0.451277                16.933333   \n",
       "4            17.0        0.055556      0.483817                10.611940   \n",
       "\n",
       "   Percentage of Complex words  Fog Index  \\\n",
       "0                     0.739631   7.529186   \n",
       "1                     0.765013   5.777434   \n",
       "2                     0.801451   6.050450   \n",
       "3                     0.767717   7.080420   \n",
       "4                     0.777778   4.555887   \n",
       "\n",
       "   average number of words per sentence  complex_count  word_counts  \\\n",
       "0                             18.083333            321          434   \n",
       "1                             13.678571            293          383   \n",
       "2                             14.324675            884         1103   \n",
       "3                             16.933333            195          254   \n",
       "4                             10.611940            553          711   \n",
       "\n",
       "   syllable count                                  PERSONAL PRONOUNS  \\\n",
       "0            1074  [they, it, it, We, there, what, their, there, ...   \n",
       "1             996  [their, them, their, it, them, its, it, Our, t...   \n",
       "2            2904  [you, you, She, it, her, her, everything, We, ...   \n",
       "3             694  [they, they, its, they, their, it, its, we, them]   \n",
       "4            1840  [we, what, I, we, our, we, us, our, what, We, ...   \n",
       "\n",
       "   average word length  \n",
       "0             6.405530  \n",
       "1             6.835509  \n",
       "2             6.813237  \n",
       "3             7.000000  \n",
       "4             6.526020  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78081190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Output Data Structure.xlsx\"\n",
    "\n",
    "submit.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
